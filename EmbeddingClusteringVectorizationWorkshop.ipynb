{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d896610",
   "metadata": {},
   "source": [
    "#### Group members\n",
    "\n",
    "Mostafa Allahmoradi - 9087818\n",
    "Jarius Bedward - 8841640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66764098c8997e0f",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b65dbfff8a6e83",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85335c3c3f9ecc4e",
   "metadata": {},
   "source": [
    "## Collection, Tokenizer, Normalization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa224cb75dc66ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51c4fd38d776c7e3",
   "metadata": {},
   "source": [
    "## Implement a Word2Vec predictive model using the knowledge corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b34425d84247c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Words2Vec\u001b[39;00m\n\u001b[32m      3\u001b[39m model_w2v = Word2Vec(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     sentences=\u001b[43mtokenized_corpus\u001b[49m, \u001b[38;5;66;03m# the tokenized corpus must be a list of lists\u001b[39;00m\n\u001b[32m      5\u001b[39m     vector_size=\u001b[32m100\u001b[39m,    \u001b[38;5;66;03m#size of embedding\u001b[39;00m\n\u001b[32m      6\u001b[39m    window=\u001b[32m5\u001b[39m,        \u001b[38;5;66;03m#context window\u001b[39;00m\n\u001b[32m      7\u001b[39m    min_count=\u001b[32m1\u001b[39m,     \u001b[38;5;66;03m#keep all words (for demo purpose\u001b[39;00m\n\u001b[32m      8\u001b[39m     workers=\u001b[32m4\u001b[39m,         \u001b[38;5;66;03m#choose how much cpu coreses use\u001b[39;00m\n\u001b[32m      9\u001b[39m   sg = \u001b[32m1\u001b[39m            \u001b[38;5;66;03m# number of skip-grams = 1 since this is small data 1\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#train model\u001b[39;00m\n\u001b[32m     13\u001b[39m model_w2v.train(tokenized_corpus, total_examples=\u001b[38;5;28mlen\u001b[39m(tokenized_corpus), epochs=\u001b[32m20\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenized_corpus' is not defined"
     ]
    }
   ],
=======
    "import string\n",
    "import nltk\n",
    "from nltk.corpus.reader import documents\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.python.types.doc_typealias import document"
   ],
   "id": "94b65dbfff8a6e83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Document Collection\n",
    "-"
   ],
   "id": "e47c0b0d6cd406be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Tokenizer, Normalization Pipeline",
   "id": "85335c3c3f9ecc4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Normalization\n",
    "\n",
    "def normalize(text):\n",
    "    # in lowercase text\n",
    "    text = text.lower()\n",
    "    #removes punctionation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    #removes numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    #Removes urls\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    #removes extra white spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "normalize_docs = [normalize(doc) for doc in documents]\n",
    "\n",
    "#Tokenization\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stopwords=set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    #remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "tokenize_docs = [tokenize(doc) for doc in normalize_docs]\n",
    "\n",
    "#FInal output print\n",
    "print(\"Original:\")\n",
    "print(documents, \"\\n\")\n",
    "\n",
    "print(\"Normalized:\")\n",
    "print(normalize_docs, \"\\n\")\n",
    "\n",
    "print(\"Tokenized:\")\n",
    "print(tokenize_docs, \"\\n\")\n"
   ],
   "id": "4aa224cb75dc66ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implement a Word2Vec predictive model using the knowledge corpus.",
   "id": "51c4fd38d776c7e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
>>>>>>> 79c080c6a0ff5cf9cf3a10067a488c226314557f
   "source": [
    "\n",
    "\n",
    "model_w2v = Word2Vec(\n",
    "    sentences=tokenized_corpus, # the tokenized corpus must be a list of lists\n",
    "    vector_size=100,    #size of embedding\n",
    "   window=5,        #context window\n",
    "   min_count=1,     #keep all words (for demo purpose\n",
    "    workers=4,         #choose how much cpu coreses use\n",
    "  sg = 1            # number of skip-grams = 1 since this is small data 1\n",
    ")\n",
    "\n",
    "#train model\n",
    "model_w2v.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=20)\n",
    "\n",
    "# ex: check similar words\n",
    "# Example: check similar words\n",
    "model_w2v.wv.most_similar(\"nlp\", topn=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06be744",
   "metadata": {},
   "source": [
    "#### Implement a GloVe count-based model using the knowledge corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f69a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install glove-python-binary\n",
    "\n",
    "from glove import Glove, Corpus\n",
    "import numpy as np\n",
    "\n",
    "sentences = [['this', 'is', 'an', 'example'], ['glove', 'is', 'awesome']]\n",
    "\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentences, window=5)\n",
    "\n",
    "glove_model = Glove(no_components=100, learning_rate=0.05)\n",
    "glove_model.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
    "glove_model.add_dictionary(corpus.dictionary)\n",
    "\n",
    "print(glove_model.word_vectors[glove_model.dictionary['glove']])\n",
    "print(glove_model.most_similar('glove'))\n",
    "\n",
    "#Download Glove Pretrained Embeddings From: http://nlp.stanford.edu/data/glove.6B.zip  \n",
    "\n",
    "def embedding_for_vocab(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "      \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n",
    "  \n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index.index(word)\n",
    "                embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "                \n",
    "    return embedding_matrix_vocab\n",
    "  \n",
    "# matrix for vocab: tokenized_words\n",
    "embedding_dim = 50\n",
    "embedding_matrix_vocab = embedding_for_vocab('../glove.6B.50d/glove.6B.50d.txt', tokenized_words, embedding_dim)\n",
    "  \n",
    "print(\"Dense vector for first word is => \", embedding_matrix_vocab[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6db5cdfd25b653",
   "metadata": {},
   "source": [
    "## ðŸ§  Learning Objectives\n",
    "- Teams of 2 (individual evaluation in class).\n",
    "- Implement **Word2Vec**  and **GloVe** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into markdown comments.\n",
    "\n",
    "\n",
    "## ðŸ§© Workshop Structure (In Class)\n",
    "1. **Set up teams of 2 people** â€“ Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Jupyter Notebook Development** *(In class)* â€“ NLP Pipeline (if needed) and Probabilistic Model method implementations + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** â€“ Teams commit and push the notebook. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around in class, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "\n",
    "\n",
    "## ðŸ’» Submission Checklist\n",
    "- âœ… `EmbeddingClusteringVectorizationWorkshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline on a relevant corpus.\n",
    "  - Demo code: Implement a Word2Vec predictive model using the knowledge corpus.\n",
    "  - Demo code: Implement a GloVe count-based model using the knowledge corpus.\n",
    "  - Markdown explanations for each major step\n",
    "  - In a table that compare **Word2Vec** against **GloVe** in the context of the use case that makes use of the knowledge corpus.\n",
    "- âœ… `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- âœ… GitHub Repo:\n",
    "  - Public repo named `EmbeddingClusteringVectorizationWorkshop`\n",
    "  - **Markdowns and meaningful talking points**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
