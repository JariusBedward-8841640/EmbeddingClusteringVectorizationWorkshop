{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Group members\n",
    "\n",
    "Mostafa Allahmoradi - 9087818\n",
    "Jarius Bedward - 8841640"
   ],
   "id": "bbaa7c9b7c213709"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports\n",
   "id": "71cf81e7a2296518"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus.reader import documents\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.python.types.doc_typealias import document"
   ],
   "id": "d0edd9ec9965389"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup",
   "id": "15f202fb127d2935"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nltk_data_path = os.path.join(os.getcwd(), \"nltk_data\")\n",
    "print(\"Downloading tokenizer resources...\")\n",
    "\n",
    "nltk.download(\"punkt\", download_dir=nltk_data_path, force=True)\n",
    "nltk.download(\"punkt_tab\", download_dir=nltk_data_path, force=True)\n",
    "\n",
    "# makes sure path is used by nltk\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "print(\"Active nltk paths:\", nltk.data.path)\n",
    "print(\"Contents of nltk_data:\", os.listdir(nltk_data_path))"
   ],
   "id": "55892887ee20316f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Document Collection\n",
    "-"
   ],
   "id": "1ffb7c99f3e8dc2b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Tokenizer, Normalization Pipeline",
   "id": "59002f9396acb348"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Normalization\n",
    "\n",
    "def normalize(text):\n",
    "    # in lowercase text\n",
    "    text = text.lower()\n",
    "    #removes punctionation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    #removes numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    #Removes urls\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    #removes extra white spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "normalize_docs = [normalize(doc) for doc in documents]\n",
    "\n",
    "#Tokenization\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stopwords=set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    #remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "tokenize_docs = [tokenize(doc) for doc in normalize_docs]\n",
    "\n",
    "#FInal output print\n",
    "print(\"Original:\")\n",
    "print(documents, \"\\n\")\n",
    "\n",
    "print(\"Normalized:\")\n",
    "print(normalize_docs, \"\\n\")\n",
    "\n",
    "print(\"Tokenized:\")\n",
    "print(tokenize_docs, \"\\n\")\n"
   ],
   "id": "31caa514b18fe811"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implement a Word2Vec predictive model using the knowledge corpus.",
   "id": "ab1cda67003e6706"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "model_w2v = Word2Vec(\n",
    "    sentences=tokenize_docs, # the tokenized corpus must be a list of lists\n",
    "    vector_size=100,    #size of embedding\n",
    "   window=5,        #context window\n",
    "   min_count=1,     #keep all words (for demo purpose\n",
    "    workers=4,         #choose how much cpu coreses use\n",
    "  sg = 1            # number of skip-grams = 1 since this is small data 1\n",
    ")\n",
    "\n",
    "#train model\n",
    "# model_w2v.train(tokenize_docs, total_examples=len(tokenize_docs), epochs=20)\n",
    "\n",
    "# ex: check similar words\n",
    "# Example: check similar words\n",
    "# model_w2v.wv.most_similar(\"nlp\", topn=3)\n",
    "\n"
   ],
   "id": "ce278b73497742e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ§  Learning Objectives\n",
    "- Teams of 2 (individual evaluation in class).\n",
    "- Implement **Word2Vec**  and **GloVe** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into markdown comments.\n",
    "\n",
    "\n",
    "## ðŸ§© Workshop Structure (In Class)\n",
    "1. **Set up teams of 2 people** â€“ Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Jupyter Notebook Development** *(In class)* â€“ NLP Pipeline (if needed) and Probabilistic Model method implementations + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** â€“ Teams commit and push the notebook. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around in class, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "\n",
    "\n",
    "## ðŸ’» Submission Checklist\n",
    "- âœ… `EmbeddingClusteringVectorizationWorkshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline on a relevant corpus.\n",
    "  - Demo code: Implement a Word2Vec predictive model using the knowledge corpus.\n",
    "  - Demo code: Implement a GloVe count-based model using the knowledge corpus.\n",
    "  - Markdown explanations for each major step\n",
    "  - In a table that compare **Word2Vec** against **GloVe** in the context of the use case that makes use of the knowledge corpus.\n",
    "- âœ… `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- âœ… GitHub Repo:\n",
    "  - Public repo named `EmbeddingClusteringVectorizationWorkshop`\n",
    "  - **Markdowns and meaningful talking points**"
   ],
   "id": "89ab523e1f9dccdb"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
