{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d896610",
   "metadata": {},
   "source": [
    "#### Group members\n",
    "\n",
    "Mostafa Allahmoradi - 9087818\n",
    "Jarius Bedward - 8841640"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports\n",
   "id": "66764098c8997e0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.python.types.doc_typealias import document"
   ],
   "id": "94b65dbfff8a6e83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Collection, Tokenizer, Normalization Pipeline",
   "id": "85335c3c3f9ecc4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Normalization\n",
    "\n",
    "def normalize(text):\n",
    "    # in lowercase text\n",
    "    text = text.lower()\n",
    "    #removes punctionation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    #removes numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    #Removes urls\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    #removes extra white spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "normalize_docs = [normalize(doc) for doc in document]"
   ],
   "id": "4aa224cb75dc66ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implement a Word2Vec predictive model using the knowledge corpus.",
   "id": "51c4fd38d776c7e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Words2Vec\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "print(\"Downloading 'punkt' tokenizer...\")\n",
    "nltk.download('punkt', download_dir=nltk_data_path, force=True)\n",
    "print(\"Downloading 'punkt_tab' tokenizer...\")\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_path, force=True)\n",
    "\n",
    "# Always append the custom nltk_data path (if not already present)\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Debugging paths and contents\n",
    "print(\"NLTK Data Paths:\", nltk.data.path)\n",
    "print(\"Contents of nltk_data:\", os.listdir(nltk_data_path))\n",
    "\n",
    "model_w2v = Word2Vec(\n",
    "    sentences=tokenized_corpus, # the tokenized corpus must be a list of lists\n",
    "    vector_size=100,    #size of embedding\n",
    "   window=5,        #context window\n",
    "   min_count=1,     #keep all words (for demo purpose\n",
    "    workers=4,         #choose how much cpu coreses use\n",
    "  sg = 1            # number of skip-grams = 1 since this is small data 1\n",
    ")\n",
    "\n",
    "#train model\n",
    "model_w2v.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=20)\n",
    "\n",
    "# ex: check similar words\n",
    "# Example: check similar words\n",
    "model_w2v.wv.most_similar(\"nlp\", topn=3)\n",
    "\n"
   ],
   "id": "f52b34425d84247c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ§  Learning Objectives\n",
    "- Teams of 2 (individual evaluation in class).\n",
    "- Implement **Word2Vec**  and **GloVe** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into markdown comments.\n",
    "\n",
    "\n",
    "## ðŸ§© Workshop Structure (In Class)\n",
    "1. **Set up teams of 2 people** â€“ Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Jupyter Notebook Development** *(In class)* â€“ NLP Pipeline (if needed) and Probabilistic Model method implementations + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** â€“ Teams commit and push the notebook. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around in class, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "\n",
    "\n",
    "## ðŸ’» Submission Checklist\n",
    "- âœ… `EmbeddingClusteringVectorizationWorkshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline on a relevant corpus.\n",
    "  - Demo code: Implement a Word2Vec predictive model using the knowledge corpus.\n",
    "  - Demo code: Implement a GloVe count-based model using the knowledge corpus.\n",
    "  - Markdown explanations for each major step\n",
    "  - In a table that compare **Word2Vec** against **GloVe** in the context of the use case that makes use of the knowledge corpus.\n",
    "- âœ… `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- âœ… GitHub Repo:\n",
    "  - Public repo named `EmbeddingClusteringVectorizationWorkshop`\n",
    "  - **Markdowns and meaningful talking points**"
   ],
   "id": "ba6db5cdfd25b653"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
