{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d896610",
   "metadata": {},
   "source": [
    "#### Group members\n",
    "\n",
    "Mostafa Allahmoradi - 9087818\n",
    "Jarius Bedward - 8841640"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports\n",
   "id": "66764098c8997e0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T19:44:59.135868Z",
     "start_time": "2025-11-25T19:44:59.130467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus.reader import documents\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.python.types.doc_typealias import document\n"
   ],
   "id": "94b65dbfff8a6e83",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup\n",
   "id": "4f58b80345d2ffd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T19:45:06.293234Z",
     "start_time": "2025-11-25T19:44:59.154093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk_data_path = os.path.join(os.getcwd(), \"nltk_data\")\n",
    "print(\"Downloading tokenizer resources...\")\n",
    "\n",
    "nltk.download(\"punkt\", download_dir=nltk_data_path, force=True)\n",
    "nltk.download(\"punkt_tab\", download_dir=nltk_data_path, force=True)\n",
    "\n",
    "# makes sure path is used by nltk\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "print(\"Active nltk paths:\", nltk.data.path)\n",
    "print(\"Contents of nltk_data:\", os.listdir(nltk_data_path))"
   ],
   "id": "1b581da7cbbafad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer resources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\jjbed\\Downloads\\ML\n",
      "[nltk_data]     prog week 13\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jjbed\\Downloads\\ML prog week 13\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active nltk paths: ['C:\\\\Users\\\\jjbed/nltk_data', 'C:\\\\Users\\\\jjbed\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data', 'C:\\\\Users\\\\jjbed\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data', 'C:\\\\Users\\\\jjbed\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\jjbed\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'C:\\\\Users\\\\jjbed\\\\Downloads\\\\ML prog week 13\\\\nltk_data']\n",
      "Contents of nltk_data: ['tokenizers']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Document Collection\n",
    "-"
   ],
   "id": "e47c0b0d6cd406be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  Tokenizer, Normalization Pipeline",
   "id": "85335c3c3f9ecc4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T19:45:06.392702Z",
     "start_time": "2025-11-25T19:45:06.325183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Normalization\n",
    "\n",
    "def normalize(text):\n",
    "    # in lowercase text\n",
    "    text = text.lower()\n",
    "    #removes punctionation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    #removes numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    #Removes urls\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    #removes extra white spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "normalize_docs = [normalize(doc) for doc in documents]\n",
    "\n",
    "#Tokenization\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stopwords=set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    #remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "tokenize_docs = [tokenize(doc) for doc in normalize_docs]\n",
    "\n",
    "#FInal output print\n",
    "print(\"Original:\")\n",
    "print(documents, \"\\n\")\n",
    "\n",
    "print(\"Normalized:\")\n",
    "print(normalize_docs, \"\\n\")\n",
    "\n",
    "print(\"Tokenized:\")\n",
    "print(tokenize_docs, \"\\n\")\n"
   ],
   "id": "4aa224cb75dc66ef",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\jjbed/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "{'coadrian.o34': 'Adrian and Ritheus', 'coaelhom.o3': 'Ã†lfric, Supplemental Homilies', 'coaelive.o3': \"Ã†lfric's Lives of Saints\", 'coalcuin': 'Alcuin De virtutibus et vitiis', 'coalex.o23': \"Alexander's Letter to Aristotle\", 'coapollo.o3': 'Apollonius of Tyre', 'coaugust': 'Augustine', 'cobede.o2': \"Bede's History of the English Church\", 'cobenrul.o3': 'Benedictine Rule', 'coblick.o23': 'Blickling Homilies', 'coboeth.o2': \"Boethius' Consolation of Philosophy\", 'cobyrhtf.o3': \"Byrhtferth's Manual\", 'cocanedgD': 'Canons of Edgar (D)', 'cocanedgX': 'Canons of Edgar (X)', 'cocathom1.o3': \"Ã†lfric's Catholic Homilies I\", 'cocathom2.o3': \"Ã†lfric's Catholic Homilies II\", 'cochad.o24': 'Saint Chad', 'cochdrul': 'Chrodegang of Metz, Rule', 'cochristoph': 'Saint Christopher', 'cochronA.o23': 'Anglo-Saxon Chronicle A', 'cochronC': 'Anglo-Saxon Chronicle C', 'cochronD': 'Anglo-Saxon Chronicle D', 'cochronE.o34': 'Anglo-Saxon Chronicle E', 'cocura.o2': 'Cura Pastoralis', 'cocuraC': 'Cura Pastoralis (Cotton)', 'codicts.o34': 'Dicts of Cato', 'codocu1.o1': 'Documents 1 (O1)', 'codocu2.o12': 'Documents 2 (O1/O2)', 'codocu2.o2': 'Documents 2 (O2)', 'codocu3.o23': 'Documents 3 (O2/O3)', 'codocu3.o3': 'Documents 3 (O3)', 'codocu4.o24': 'Documents 4 (O2/O4)', 'coeluc1': 'Honorius of Autun, Elucidarium 1', 'coeluc2': 'Honorius of Autun, Elucidarium 1', 'coepigen.o3': \"Ã†lfric's Epilogue to Genesis\", 'coeuphr': 'Saint Euphrosyne', 'coeust': 'Saint Eustace and his companions', 'coexodusP': 'Exodus (P)', 'cogenesiC': 'Genesis (C)', 'cogregdC.o24': \"Gregory's Dialogues (C)\", 'cogregdH.o23': \"Gregory's Dialogues (H)\", 'coherbar': 'Pseudo-Apuleius, Herbarium', 'coinspolD.o34': \"Wulfstan's Institute of Polity (D)\", 'coinspolX': \"Wulfstan's Institute of Polity (X)\", 'cojames': 'Saint James', 'colacnu.o23': 'Lacnunga', 'colaece.o2': 'Leechdoms', 'colaw1cn.o3': 'Laws, Cnut I', 'colaw2cn.o3': 'Laws, Cnut II', 'colaw5atr.o3': 'Laws, Ã†thelred V', 'colaw6atr.o3': 'Laws, Ã†thelred VI', 'colawaf.o2': 'Laws, Alfred', 'colawafint.o2': \"Alfred's Introduction to Laws\", 'colawger.o34': 'Laws, Gerefa', 'colawine.ox2': 'Laws, Ine', 'colawnorthu.o3': 'Northumbra Preosta Lagu', 'colawwllad.o4': 'Laws, William I, Lad', 'coleofri.o4': 'Leofric', 'colsigef.o3': \"Ã†lfric's Letter to Sigefyrth\", 'colsigewB': \"Ã†lfric's Letter to Sigeweard (B)\", 'colsigewZ.o34': \"Ã†lfric's Letter to Sigeweard (Z)\", 'colwgeat': \"Ã†lfric's Letter to Wulfgeat\", 'colwsigeT': \"Ã†lfric's Letter to Wulfsige (T)\", 'colwsigeXa.o34': \"Ã†lfric's Letter to Wulfsige (Xa)\", 'colwstan1.o3': \"Ã†lfric's Letter to Wulfstan I\", 'colwstan2.o3': \"Ã†lfric's Letter to Wulfstan II\", 'comargaC.o34': 'Saint Margaret (C)', 'comargaT': 'Saint Margaret (T)', 'comart1': 'Martyrology, I', 'comart2': 'Martyrology, II', 'comart3.o23': 'Martyrology, III', 'comarvel.o23': 'Marvels of the East', 'comary': 'Mary of Egypt', 'coneot': 'Saint Neot', 'conicodA': 'Gospel of Nicodemus (A)', 'conicodC': 'Gospel of Nicodemus (C)', 'conicodD': 'Gospel of Nicodemus (D)', 'conicodE': 'Gospel of Nicodemus (E)', 'coorosiu.o2': 'Orosius', 'cootest.o3': 'Heptateuch', 'coprefcath1.o3': \"Ã†lfric's Preface to Catholic Homilies I\", 'coprefcath2.o3': \"Ã†lfric's Preface to Catholic Homilies II\", 'coprefcura.o2': 'Preface to the Cura Pastoralis', 'coprefgen.o3': \"Ã†lfric's Preface to Genesis\", 'copreflives.o3': \"Ã†lfric's Preface to Lives of Saints\", 'coprefsolilo': \"Preface to Augustine's Soliloquies\", 'coquadru.o23': 'Pseudo-Apuleius, Medicina de quadrupedibus', 'corood': 'History of the Holy Rood-Tree', 'cosevensl': 'Seven Sleepers', 'cosolilo': \"St. Augustine's Soliloquies\", 'cosolsat1.o4': 'Solomon and Saturn I', 'cosolsat2': 'Solomon and Saturn II', 'cotempo.o3': \"Ã†lfric's De Temporibus Anni\", 'coverhom': 'Vercelli Homilies', 'coverhomE': 'Vercelli Homilies (E)', 'coverhomL': 'Vercelli Homilies (L)', 'covinceB': 'Saint Vincent (Bodley 343)', 'covinsal': 'Vindicta Salvatoris', 'cowsgosp.o3': 'West-Saxon Gospels', 'cowulf.o34': \"Wulfstan's Homilies\"} \n",
      "\n",
      "Normalized:\n",
      "['coadriano', 'coaelhomo', 'coaeliveo', 'coalcuin', 'coalexo', 'coapolloo', 'coaugust', 'cobedeo', 'cobenrulo', 'coblicko', 'coboetho', 'cobyrhtfo', 'cocanedgd', 'cocanedgx', 'cocathomo', 'cocathomo', 'cochado', 'cochdrul', 'cochristoph', 'cochronao', 'cochronc', 'cochrond', 'cochroneo', 'cocurao', 'cocurac', 'codictso', 'codocuo', 'codocuo', 'codocuo', 'codocuo', 'codocuo', 'codocuo', 'coeluc', 'coeluc', 'coepigeno', 'coeuphr', 'coeust', 'coexodusp', 'cogenesic', 'cogregdco', 'cogregdho', 'coherbar', 'coinspoldo', 'coinspolx', 'cojames', 'colacnuo', 'colaeceo', 'colawcno', 'colawcno', 'colawatro', 'colawatro', 'colawafo', 'colawafinto', 'colawgero', 'colawineox', 'colawnorthuo', 'colawwllado', 'coleofrio', 'colsigefo', 'colsigewb', 'colsigewzo', 'colwgeat', 'colwsiget', 'colwsigexao', 'colwstano', 'colwstano', 'comargaco', 'comargat', 'comart', 'comart', 'comarto', 'comarvelo', 'comary', 'coneot', 'conicoda', 'conicodc', 'conicodd', 'conicode', 'coorosiuo', 'cootesto', 'coprefcatho', 'coprefcatho', 'coprefcurao', 'coprefgeno', 'coprefliveso', 'coprefsolilo', 'coquadruo', 'corood', 'cosevensl', 'cosolilo', 'cosolsato', 'cosolsat', 'cotempoo', 'coverhom', 'coverhome', 'coverhoml', 'covinceb', 'covinsal', 'cowsgospo', 'cowulfo'] \n",
      "\n",
      "Tokenized:\n",
      "[['coadriano'], ['coaelhomo'], ['coaeliveo'], ['coalcuin'], ['coalexo'], ['coapolloo'], ['coaugust'], ['cobedeo'], ['cobenrulo'], ['coblicko'], ['coboetho'], ['cobyrhtfo'], ['cocanedgd'], ['cocanedgx'], ['cocathomo'], ['cocathomo'], ['cochado'], ['cochdrul'], ['cochristoph'], ['cochronao'], ['cochronc'], ['cochrond'], ['cochroneo'], ['cocurao'], ['cocurac'], ['codictso'], ['codocuo'], ['codocuo'], ['codocuo'], ['codocuo'], ['codocuo'], ['codocuo'], ['coeluc'], ['coeluc'], ['coepigeno'], ['coeuphr'], ['coeust'], ['coexodusp'], ['cogenesic'], ['cogregdco'], ['cogregdho'], ['coherbar'], ['coinspoldo'], ['coinspolx'], ['cojames'], ['colacnuo'], ['colaeceo'], ['colawcno'], ['colawcno'], ['colawatro'], ['colawatro'], ['colawafo'], ['colawafinto'], ['colawgero'], ['colawineox'], ['colawnorthuo'], ['colawwllado'], ['coleofrio'], ['colsigefo'], ['colsigewb'], ['colsigewzo'], ['colwgeat'], ['colwsiget'], ['colwsigexao'], ['colwstano'], ['colwstano'], ['comargaco'], ['comargat'], ['comart'], ['comart'], ['comarto'], ['comarvelo'], ['comary'], ['coneot'], ['conicoda'], ['conicodc'], ['conicodd'], ['conicode'], ['coorosiuo'], ['cootesto'], ['coprefcatho'], ['coprefcatho'], ['coprefcurao'], ['coprefgeno'], ['coprefliveso'], ['coprefsolilo'], ['coquadruo'], ['corood'], ['cosevensl'], ['cosolilo'], ['cosolsato'], ['cosolsat'], ['cotempoo'], ['coverhom'], ['coverhome'], ['coverhoml'], ['covinceb'], ['covinsal'], ['cowsgospo'], ['cowulfo']] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jjbed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implement a Word2Vec predictive model using the knowledge corpus.",
   "id": "51c4fd38d776c7e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T19:45:06.452131Z",
     "start_time": "2025-11-25T19:45:06.407781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "model_w2v = Word2Vec(\n",
    "    sentences=tokenize_docs, # the tokenized corpus must be a list of lists\n",
    "    vector_size=100,    #size of embedding\n",
    "   window=5,        #context window\n",
    "   min_count=1,     #keep all words (for demo purpose\n",
    "    workers=4,         #choose how much cpu coreses use\n",
    "  sg = 1            # number of skip-grams = 1 since this is small data 1\n",
    ")\n",
    "\n",
    "#train model\n",
    "# model_w2v.train(tokenize_docs, total_examples=len(tokenize_docs), epochs=20)\n",
    "\n",
    "# ex: check similar words\n",
    "# Example: check similar words\n",
    "# model_w2v.wv.most_similar(\"nlp\", topn=3)\n",
    "\n"
   ],
   "id": "f52b34425d84247c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ§  Learning Objectives\n",
    "- Teams of 2 (individual evaluation in class).\n",
    "- Implement **Word2Vec**  and **GloVe** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into markdown comments.\n",
    "\n",
    "\n",
    "## ðŸ§© Workshop Structure (In Class)\n",
    "1. **Set up teams of 2 people** â€“ Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Jupyter Notebook Development** *(In class)* â€“ NLP Pipeline (if needed) and Probabilistic Model method implementations + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** â€“ Teams commit and push the notebook. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around in class, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "\n",
    "\n",
    "## ðŸ’» Submission Checklist\n",
    "- âœ… `EmbeddingClusteringVectorizationWorkshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline on a relevant corpus.\n",
    "  - Demo code: Implement a Word2Vec predictive model using the knowledge corpus.\n",
    "  - Demo code: Implement a GloVe count-based model using the knowledge corpus.\n",
    "  - Markdown explanations for each major step\n",
    "  - In a table that compare **Word2Vec** against **GloVe** in the context of the use case that makes use of the knowledge corpus.\n",
    "- âœ… `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- âœ… GitHub Repo:\n",
    "  - Public repo named `EmbeddingClusteringVectorizationWorkshop`\n",
    "  - **Markdowns and meaningful talking points**"
   ],
   "id": "ba6db5cdfd25b653"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
